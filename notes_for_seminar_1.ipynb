{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64297ddd-b7ba-4c1e-a1bf-ba045821a114",
   "metadata": {},
   "source": [
    "### 1. Подключение библиотек\n",
    "Наличие большого числа разносторонних библиотек - одно из ключевых преимуществ языка Python  \n",
    "Среди огромного количества библиотек стоит выделить следующие:\n",
    "* numpy - библиотека для работы с массивами/матрицами\n",
    "* pandas - библиотека для работы с таблицами\n",
    "* RDKit - библиотека для работы с молекулами и хим. сущностями\n",
    "* matplotlib - библиотека для визуализации\n",
    "* psi4 - квантовые расчеты\n",
    "* openmm - мол. динамика\n",
    "* pickle - библиотека для сохранения объектов в бинарных файлах\n",
    "* os - для работы с файловой системой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b28d3-715f-4367-a0f6-f00760b4f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pandas понадобится нам для работы с данными\n",
    "import pandas as pd\n",
    "### Различные подмодули из rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "### для сохранения в файл\n",
    "import pickle\n",
    "### для работы с файловой системой\n",
    "import os\n",
    "### для отрисовки\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import NonUniformImage\n",
    "### Для работы с массивами\n",
    "import numpy as np\n",
    "### Импорты для анализа потраченной памяти\n",
    "from sys import getsizeof\n",
    "import psutil\n",
    "\n",
    "### Переменная, указывающая папку с нашими файлами\n",
    "data_folder = \"/home/alex/ml_lectures\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcf2e2-ed00-475a-a244-27fb2843d862",
   "metadata": {},
   "source": [
    "### 2. Загрузка исходных данных\n",
    "#### Что представляют данные в датасете\n",
    "    I.  Property  Unit         Description\n",
    "    --  --------  -----------  --------------\n",
    "     1  tag       -            \"gdb9\"; string constant to ease extraction via grep\n",
    "     2  index     -            Consecutive, 1-based integer identifier of molecule\n",
    "     3  A         GHz          Rotational constant A\n",
    "     4  B         GHz          Rotational constant B\n",
    "     5  C         GHz          Rotational constant C\n",
    "     6  mu        Debye        Dipole moment\n",
    "     7  alpha     Bohr^3       Isotropic polarizability\n",
    "     8  homo      Hartree      Energy of Highest occupied molecular orbital (HOMO)\n",
    "     9  lumo      Hartree      Energy of Lowest occupied molecular orbital (LUMO)\n",
    "    10  gap       Hartree      Gap, difference between LUMO and HOMO\n",
    "    11  r2        Bohr^2       Electronic spatial extent\n",
    "    12  zpve      Hartree      Zero point vibrational energy\n",
    "    13  U0        Hartree      Internal energy at 0 K\n",
    "    14  U         Hartree      Internal energy at 298.15 K\n",
    "    15  H         Hartree      Enthalpy at 298.15 K\n",
    "    16  G         Hartree      Free energy at 298.15 K\n",
    "    17  Cv        cal/(mol K)  Heat capacity at 298.15 K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed1d6f-617f-4ef0-8cee-e7ac9c5c4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### В прошлой раз мы разобрались с загрузкой. Просто выгружаем данные из csv\n",
    "df1 = pd.read_csv(os.path.join(data_folder,\"qm9.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99220ecc-a427-42ef-8a9f-c732cd990a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### удалем столбец mol_id\n",
    "\n",
    "### Проверка на то, что столбец -- это обычная индексация без проблемных мест\n",
    "check_val = (df1[\"mol_id\"].apply(lambda x: int(x[4:]) - 1) - np.arange(df1.shape[0])).abs().sum()\n",
    "print( \"mol_id просто индексатор\" if check_val==0 else \"С mol_id что-то интересное\")\n",
    "### С чистой совестью можем удалять mol_id\n",
    "### inplace указывает на то, что мы просим сделать это на месте,\n",
    "### а не возвращать копию. См. help(df1.drop)\n",
    "df1.drop(columns=\"mol_id\", inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6841768-91c3-4555-ba05-3f7e5c5d4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Очень важно при отборе фичей для ML избегать сильных корреляций.\n",
    "### Взглянем на парные корреляции по kendall внутри датасета\n",
    "corr_map = df1[list(df1)[1:]].corr(method='kendall')\n",
    "corr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36754743-75d0-4b22-9564-eb1a9c0f7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_np = np.triu(corr_map.to_numpy(), k=1)\n",
    "idx = np.unravel_index(corr_np.argmax(), corr_np.shape)\n",
    "print(\"Максимальная корреляция между столбцами {} и {} и равна {:.10f}\".format( \n",
    "            list(corr_map)[idx[0]], list(corr_map)[idx[1]], corr_map.iloc[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1366d9-3709-41a8-8b7f-f9d5b31be576",
   "metadata": {},
   "source": [
    "При построении сложных моделей __обычно отказываются от столбцов при корелляции более 0.98 или 0.95.__  \n",
    "Отметим, что более многомерные корелляции здесь не проверяются. Не сложно понять, что gap = lumo - homo  \n",
    "Но такого мы таким простым способом не выявим, к сожалению  \n",
    "**<u> ДЗ: почистить датасет, чтобы максимальная корреляция по Кендаллу между столбцами была меньше 0.97</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a0c63-56d7-48f3-89ba-bb481bd0f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ДЗ, чистим матрицу\n",
    "### Берез матрицу абсолютных значений парных корелляций\n",
    "cor_matrix = df1.iloc[:,1:].corr(method='kendall').abs()\n",
    "### Выбираем верхний треугольник данной матрицы (без диагонали)\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "### Выбирвем столбцы для удаления\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.97)]\n",
    "### Выкидываем столбцы, и создаем новое представление df1_nocorr\n",
    "df1_nocorr = df1.drop(columns=to_drop)\n",
    "### Выкидываем столбец gap, используя эмпирические знания о том, что gap = lumo - homo\n",
    "df1_nocorr = df1_nocorr.drop(columns=\"gap\")\n",
    "### Посмотрим на результат\n",
    "df1_nocorr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9c9ef-7b2c-44d6-8610-e538131afd6c",
   "metadata": {},
   "source": [
    "### 3. Предсказываем $r^2$\n",
    "#### __3.1 Разделение данных__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92063610-f1bb-4cff-b806-ea44323cf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Стандартная функция для разделения датасета\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec079d40-47dc-4c3e-b86a-846196167414",
   "metadata": {},
   "source": [
    "Итак, в следующих ячейках мы воспользуемся функцией __train_test_split дважды подряд__  \n",
    "Почему и что важно помнить!!! У нас есть 2 базыовые части датасета:\n",
    "* __Тренировочная часть__, доступная нам в ходе поиска и отпимизации моделей машинного обучения.\n",
    "* __Тестовая часть__, которую __нельзя использовать ни для сравнения моделей, ни для их оптимизации__.  \n",
    "  Это часть данных используется лишь раз при представлении финальных результатов на выбранной, готовой модели.  \n",
    "  Эти данные мы отделим в самом начале и не будем к ним прикасаться почти до конца. Назовем их __outer_test__.  \n",
    "  \n",
    "Для того, чтобы искать лучшую модель, мы отделим от тренировочного еще кусок,  \n",
    "он будет указан здесь как __test__ лишь в силу исторических причин, но по факту и духу - __это валидационный датасет__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8542a-cbb5-48fc-bce9-511b73fd8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделяем наш массив на тренировочную+валидационную и тестовую выборки\n",
    "### test_size указывает, что мы хотим 15% данных в качестве тестовых\n",
    "### random_state позволяет указывать выбранное случайное значение при изначальном\n",
    "###    перемешивании строк для воспроизводимости результатов\n",
    "train_and_valid, outer_test = train_test_split(df1_nocorr, test_size = 0.15, random_state=1234)\n",
    "\n",
    "### Взглянем на train\n",
    "train_and_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8319502-8644-49d2-9fa1-8c525d73ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10% на валидацию\n",
    "train, test = train_test_split(train_and_valid, test_size = 0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d3d8c-d61a-4e75-a2bd-d1b7a5dfdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Теперь нам нужно выделить X, y -- данные на которых базируются \n",
    "### предсказания и данные, которые мы хотим предсказывать\n",
    "col_name = \"r2\"\n",
    "X_train = train[[x for x in train if x!=\"smiles\" and x!=col_name]]\n",
    "y_train = train[col_name]\n",
    "X_test = test[[x for x in test if x!=\"smiles\" and x!=col_name]]\n",
    "y_test = test[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b637d6-9f0c-447f-8a53-b7e3fc26142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделить данные можно также в другом порядке\n",
    "X = train_and_valid[[x for x in df1_nocorr if x!=\"smiles\" and x!=col_name]]\n",
    "y = train_and_valid[col_name]\n",
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(X, y, test_size = 0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc5e5e-2d6f-4f69-8a1c-7738a90b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проверим идентичность разбиений при смене порядка действий\n",
    "### Отметим, что равенство достигается только при заданном random_state\n",
    "print( \"Разбиения идентичны\" if (X_train.equals(X_train_v2) and X_test.equals(X_test_v2) \\\n",
    "                                 and y_train.equals(y_train_v2) and y_test.equals(y_test_v2)) \\\n",
    "      else \"Разбиения НЕ ОДИНАКОВЫ !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8b1d7-412f-4b6f-a310-d14604e5f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "### В самом простом случае за разбиением данных следует создание модели,\n",
    "### ее обучение и использование для предсказаний с предварительным тестированием\n",
    "### Восползуемся простейшим линейным регрессором\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "### Стандартные метрики для анализа качества регрессионных моделей\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834d39b-a417-4f88-911b-05a5de9d4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### создаем объект класса линейного регрессора\n",
    "lr = LR()\n",
    "### Обучаем на тренировочных данных\n",
    "lr.fit(X=X_train, y=y_train)\n",
    "### Сохраняем предсказания\n",
    "preds = lr.predict(X_test)\n",
    "print(\"Результаты на тесте: MAE = {:.2f}, RMSE = {:.2f}\".\n",
    "      format( mean_absolute_error(preds, y_test),\n",
    "              mean_squared_error(preds, y_test) ))\n",
    "\n",
    "### Для одиночной модели, эти данные чуть более чем бесполезны\n",
    "### Куда приятнее взглянуть на относительные погрешности, \n",
    "### но с ними нужно быть крайне аккуратными и следить, на что вы делите\n",
    "print( \"В столбце r2 ровно {} строк со значением меньше 0.1\".\n",
    "      format(df1_nocorr[df1_nocorr['r2']<0.1].shape[0]))\n",
    "print( \"Средняя относительная погрешность = {:.2f}%\".format( ((preds-y_test)/y_test).abs().mean()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db3f02-2a36-443f-96a3-920ea884cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(preds,yt, check_col=None, df=None):\n",
    "    print(\"Результаты на тесте: MAE = {:.2f}, RMSE = {:.2f}\".\n",
    "          format( mean_absolute_error(preds, yt),\n",
    "                  mean_squared_error(preds, yt) ))\n",
    "    if check_col is not None:\n",
    "    ### Для одиночной модели, эти данные чуть более чем бесполезны\n",
    "    ### Куда приятнее взглянуть на относительные погрешности, \n",
    "    ### но с ними нужно быть крайне аккуратными и следить, на что вы делите\n",
    "        print( \"В столбце \"+check_col+\" ровно {} строк со значением меньше 0.1\".\n",
    "              format(df[df[check_col]<0.1].shape[0]))\n",
    "    print( \"Средняя относительная погрешность = {:.2f}%\".format( ((preds-yt)/yt).abs().mean()*100 ))\n",
    "    \n",
    "\n",
    "### Стоит оформить это в функцию:\n",
    "def fit_pred(X,Xt,y,yt,model, df=df1_nocorr, col='r2'):\n",
    "    ### Обучаем на тренировочных данных\n",
    "    model.fit(X=X, y=y)\n",
    "    ### Сохраняем предсказания\n",
    "    preds = model.predict(Xt)\n",
    "    print_metrics(preds,yt,check_col=col,df=df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63e28a-55dc-46f1-b38c-a3b9b7a5e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Взглянем на коэффициенты линейной модели\n",
    "print(\"Linear model coefficients:\\n\")\n",
    "for i in range(X_test.shape[1]):\n",
    "    print(\" {:>7} = {:>10.4f} \".format(X_test.columns[i],  lr.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec2693-ba6f-4fa3-81c7-9e9d1cd57828",
   "metadata": {},
   "source": [
    "Без нормализации исходного вектора, эти числа не в полной мере отражают важность признаков  \n",
    "Попробуем, по-быстрому, посмотреть на результат SHAP-анализа.  \n",
    "На графике ниже показано насколько сильно тот или иной вклад фичи меняет вклад для каждой точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efd6bc-2764-4521-aafd-8b625e73152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### библиотека для shap-анализа\n",
    "import shap\n",
    "explainer = shap.Explainer(lr.predict, X_test)\n",
    "### Используем только 2000 строк, ибо долго\n",
    "shap_values = explainer(X_train.head(2000))\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c295297-6f43-4ccb-9943-c27502aec87a",
   "metadata": {},
   "source": [
    "Видим, что можно выкинуть колонку А и предсказание не поменяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d336-fcdd-4bef-9f3d-1e06a7b3310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Выкинем колонку А и проверим предсказание\n",
    "_ = fit_pred(X_train.drop(columns='A'),X_test.drop(columns='A'),y_train,y_test, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae97225-4ce8-4cf9-afdc-5c5411c28501",
   "metadata": {},
   "source": [
    "__Важное замечание__  \n",
    "Линейный регрессор - простая, быстрая и пригодная модель для подобного анализа.  \n",
    "Однако стоит помнить, данные результаты -- это  не гарантия того,  \n",
    "что выкинутая фича не окажется полезной в другой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1c882-9119-4bba-b99d-a64a793fb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Импорт из sklearn различных полезностей\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, ConstantKernel\n",
    "# from sklearn.tree import ExtraTreeRegressor, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fd871-da86-4275-b568-33a43f892da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### offtopic: класс для удобного выделения текста в print\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b85a7-9d9c-42dd-a337-09f5d5e17545",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Название классов моделей\n",
    "names_solvers = [\"XGB\", \"KNeighborsRegressor\", \"SVR_Sigmoid\",\n",
    "                 \"Linear_Regression\", \"MLP\"]\n",
    "### Генерируем объекты классов\n",
    "xgb = XGBRegressor()\n",
    "knn = KNeighborsRegressor()\n",
    "svr_sigma = SVR(kernel=\"sigmoid\", max_iter=400, C=100 ,cache_size=1000)\n",
    "lr = LR()\n",
    "ml = MLPRegressor(hidden_layer_sizes=(64,32,16,8), activation='relu', max_iter=200)\n",
    "### Заносим их в массив\n",
    "solvers = [xgb, knn, svr_sigma, lr, ml ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9262f-d3dd-4ca9-8fc2-57260a628efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 цикла. Внутри каждого цикла запускаем все модели\n",
    "### Обратите внимание на использование deepcopy\n",
    "### Не самое удачное решение, но в ином случае модель во втором цикле\n",
    "###   уже окажется обученной ((\n",
    "\n",
    "### На обычном нетронутом датасете \n",
    "print(color.BOLD + \"Без изменений датасета\" + color.END)\n",
    "for solv, name in zip(solvers, names_solvers):\n",
    "    print(\"Start with \"+name)\n",
    "    solver_copy = cp(solv)\n",
    "    preds = solver_copy.fit(X_train, y_train).predict(X_test)\n",
    "    print_metrics(preds,y_test)\n",
    "    print()\n",
    "print()\n",
    "### Перед моделью стоит нормализатор. Он переводит столбцы в более приглядный\n",
    "###   для некоторых моделей вид. \n",
    "print(color.BOLD + \"С использованием перенормировки\" + color.END)\n",
    "for solv, name in zip(solvers, names_solvers):\n",
    "    print(\"Start with \"+name)\n",
    "    solver_copy = cp(solv)\n",
    "    model = make_pipeline(StandardScaler(), solver_copy)\n",
    "    preds = model.fit(X_train, y_train).predict(X_test)\n",
    "    print_metrics(preds,y_test)\n",
    "    print()\n",
    "print()\n",
    "### Теперь не только нормализуются данные, но и генерируются квадраты фичей\n",
    "print(color.BOLD + \"С использованием перенормировки и квадратичных фичей\" + color.END)\n",
    "for solv, name in zip(solvers, names_solvers):\n",
    "    print(\"Start with \"+name)\n",
    "    solver_copy = cp(solv)\n",
    "    model = make_pipeline(StandardScaler(), PolynomialFeatures(), solver_copy)\n",
    "    preds = model.fit(X_train, y_train).predict(X_test)\n",
    "    print_metrics(preds,y_test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efc03f-53e2-4631-b1c1-99d2cff85e15",
   "metadata": {},
   "source": [
    "__<u> ДЗ: </u> на примере столбца u0_atom показать как работает StandardScaler и PolynomialFeatures (ответ с графиками или гистограммами)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff9b4f-69a2-4c5c-8298-ab4d3822bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Давайте взглянем какие фичи XGBoost выделяет как важные\n",
    "### Обучим модель и восползуемся переменной feature_importances_\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "preds = xgb.predict(X_test)\n",
    "print_metrics(y_test,preds)\n",
    "ax1 = plt.subplot()\n",
    "ax1.set_xticks(range(len(xgb.feature_importances_)))\n",
    "plt.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\n",
    "ax1.set_xticklabels(list(X_train))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cca807-8e7d-4dc0-b07e-3d98fc86490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Что будет если взять только пару топовых фичей для XGBoost\n",
    "arr = []\n",
    "for feature in X_train.iloc[:,xgb.feature_importances_.argsort()[::-1][:5]].columns:\n",
    "    arr.append(feature)\n",
    "    xgb = XGBRegressor()\n",
    "    preds = xgb.fit(X_train[arr], y_train).predict(X_test[arr])\n",
    "    print(\"XGB with {} top features: \".format(len(arr)) + \" \".join(arr))\n",
    "    print_metrics(y_test,preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f957b7b-2e26-415d-81ba-d483cc7ef2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Если поменять random_state, то результаты на валидационных датасетах\n",
    "###  могут отличаться друг от друга.\n",
    "### Исходно было:\n",
    "# Результаты на тесте: MAE = 10.41, RMSE = 191.96\n",
    "# Средняя относительная погрешность = 0.87%\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, test_size = 0.1, random_state=432101234)\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train_new, y_train_new)\n",
    "preds = xgb.predict(X_test_new)\n",
    "print_metrics(y_test_new,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f908d-9db1-4b49-9695-436e836fcc8a",
   "metadata": {},
   "source": [
    "Чтобы иметь __статистически сформулированный ответ о точнотсти предсказания__  \n",
    "можно воспользоваться Кросс-Валидацией с повторением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86394281-56ab-4be2-901c-8bbbc5f42147",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Кросс-валидация с повторением\n",
    "### Прекрасный код, почти не тронут из мануала sklearn\n",
    "\n",
    "from scipy.stats import sem\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, model_func, repeats=3):\n",
    "    # prepare the cross-validation procedure\n",
    "    model = model_func()\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# configurations to test\n",
    "repeats = 3\n",
    "results = dict()\n",
    "for model_func,name in zip([XGBRegressor, KNeighborsRegressor],[\"XGBoostRegr\", \"KNeighborsRegressor\"]):\n",
    "    # evaluate using a given number of repeats\n",
    "    scores = evaluate_model(X, y, model_func, repeats=repeats)\n",
    "    # summarize\n",
    "    print(name)\n",
    "    print('>%d mean=%.4f se=%.3f' % (repeats, mean(scores), sem(scores)))\n",
    "    print()\n",
    "    # store\n",
    "    results[name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30d0c7-ed99-4cb0-872b-06d467b3fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(results.values(), labels=results.keys(), showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc10f3-787a-4465-82b8-9055e54c44ab",
   "metadata": {},
   "source": [
    "__<u> ДЗ: </u>__ Выше не оказалось MLP по 2 причинам:\n",
    "1. MLP очень долго считается.\n",
    "2. MLP нужны аргументы.  \n",
    "\n",
    "Задание в том, чтобы, сохраняя передачу в evaluate_model класса модели,  \n",
    "занести вовнутрь нужные параметры и указать их при инициализации объекта модели.  \n",
    "Подсказка: смотрите в сторону \\*args и \\*\\*kargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f8bf7-4bb1-4826-8db1-273327bc868e",
   "metadata": {},
   "source": [
    "__Почему стоит избавляться от скореллированных столбцов__  \n",
    "Рассмотрим линейную модель со всеми столбцами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0e502-3ceb-458b-88df-51d056353a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Берем все столбцы\n",
    "### И делем все при тех же random_state (и тем же количеством строк)\n",
    "train_and_valid_full, outer_test_full = train_test_split(df1, test_size = 0.15, random_state=1234)\n",
    "X_full = train_and_valid_full[[x for x in df1 if x!=\"smiles\" and x!=col_name]]\n",
    "y_full = train_and_valid_full[col_name]\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size = 0.1, random_state=1234)\n",
    "\n",
    "lr = LR()\n",
    "lr = fit_pred(X_train_full, X_test_full, y_train_full, y_test_full, lr)\n",
    "### Взглянем на коэффициенты линейной модели\n",
    "print(\"Linear model coefficients:\\n\")\n",
    "for i in range(X_test_full.shape[1]):\n",
    "    print(\" {:>10} = {:>15.4f} \".format(X_test_full.columns[i],  lr.coef_[i]))\n",
    "### библиотека для shap-анализа\n",
    "import shap\n",
    "explainer = shap.Explainer(lr.predict, X_test_full)\n",
    "### Используем только 2000 строк, ибо долго\n",
    "shap_values = explainer(X_train_full.head(2000))\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b9df5-f43d-4b35-8524-257dd5f1007c",
   "metadata": {},
   "source": [
    "Вряд ли такие коэффициенты -- это наш эталон устойчивой системы.  \n",
    "Стоит аккуратнее относится к этим результатом, даже если они стали лучше  \n",
    "При этом xgb не изменилась практически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57d793-2767-401e-a3bb-0a7b92430cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor()\n",
    "xgbr = fit_pred(X_train_full, X_test_full, y_train_full, y_test_full, xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987be92f-061e-48de-ab94-19fa4e07ef02",
   "metadata": {},
   "source": [
    "### Финальные результате на внешнем тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfe824-50c4-4f19-81de-661018308b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor()\n",
    "print_metrics( xgbr.fit(X,y).predict(\n",
    "    outer_test[[x for x in outer_test if x!='smiles' and x!='r2']]), outer_test['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41935525-e9e1-4167-b115-0da1b4b81cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
